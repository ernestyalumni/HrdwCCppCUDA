% file: HrdwCCppCUDA_grande.tex
% Computational Physics (notes), in unconventional ``grande'' format; fitting a widescreen format
% 
% github        : ernestyalumni
% gmail         : ernestyalumni 
% linkedin      : ernestyalumni 
% wordpress.com : ernestyalumni
%
% This code is open-source, governed by the Creative Common license.  Use of this code is governed by the Caltech Honor Code: ``No member of the Caltech community shall take unfair advantage of any other member of the Caltech community.'' 

\documentclass[10pt]{amsart}
\pdfoutput=1
%\usepackage{mathtools,amssymb,lipsum,caption}
\usepackage{mathtools,amssymb,caption}


\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{pdfpages}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}

\usepackage{multicol}

\hypersetup{colorlinks=true,citecolor=[rgb]{0,0.4,0}}

\oddsidemargin=15pt
\evensidemargin=5pt
\hoffset-45pt
\voffset-55pt
\topmargin=-4pt
\headsep=5pt
\textwidth=1120pt
\textheight=595pt
\paperwidth=1200pt
\paperheight=700pt
\footskip=40pt








\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
%\newtheorem*{main}{Main Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

%This defines a new command \questionhead which takes one argument and
%prints out Question #. with some space.
\newcommand{\questionhead}[1]
  {\bigskip\bigskip
   \noindent{\small\bf Question #1.}
   \bigskip}

\newcommand{\problemhead}[1]
  {
   \noindent{\small\bf Problem #1.}
   }

\newcommand{\exercisehead}[1]
  { \smallskip
   \noindent{\small\bf Exercise #1.}
  }

\newcommand{\solutionhead}[1]
  {
   \noindent{\small\bf Solution #1.}
   }


  \title[Hardware (Compilers, OS, runtime) and C, C++, to CUDA]{Hardware (Compilers, OS, runtime) and C, C++, to CUDA}
\author{Ernest Yeung \href{mailto:ernestyalumni@gmail.com}{ernestyalumni@gmail.com}}
\date{1 Nov 2017}
\keywords{C, C++, CUDA, CUDA C/C++, Compilers, OS, runtime, classes, Object Oriented Programming, Segmentation Fault, memory, memory addresses}

\begin{document}

\definecolor{darkgreen}{rgb}{0,0.4,0}
\lstset{language=C++,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
 commentstyle=\color{darkgreen}
 }
%\lstlistoflistings

\maketitle

\tableofcontents


\begin{multicols*}{2}

\begin{abstract}
I review what, how, and why Segmentation Fault is and occurs in the specific context of C and hopefully to CUDA, virtual memory addresses in C++, and to CUDA.  


\end{abstract}

\section{Introduction; why I'm writing this}  

I didn't realize the importance of having a profound understanding of C/C++ and its relation to hardware, in particular memory (addresses), until I learned more specifically about the work done at NVIDIA from its news and job postings.  Coming from a theoretical and mathematical physics background, I wanted to get myself up to speed as fast as possible, provide good textbook and online references, and directly relate these topics, which seem to be classic topics in computer science (at hardware level) and electrical engineering, to CUDA, to specifically GPU parallel programming with direct CUDA examples.  

Note that there is a version of this LaTeX/pdf file in the usual "letter" format that is exactly the same as the content here, \verb|HrdwCCppCUDA.pdf|.  

Meyer (1997) \cite{Meye1997}


\begin{quotation}
"When programmers first began giving up assembly language in favor of using HLLs, they generally understood the low-level ramifications of the HLL statements they were using and could choose their HLL statements appropriately.Unfortunately, the generation of computer programmers that followed them did not have the benefit of mastering assembly language. As such, they were not in a position to wisely choose statements and data structures that HLLs could efficiently translate into machine code."  
\end{quotation}
pp. 2, Ch. 1 of Hyde (2006) \cite{Hyde2006}, where HLL stands for high-level language.  I was part of that generation and knew of nothing of assembly.  

\section{80x86 Assembly; 80x86 Architecture}  

cf. Ch. 3 of Hyde (2006) \cite{Hyde2006}

Main difference between complex instruction set computer (CISC) architectures such as 80x86 and reduced instruction set computer (RISC) architectures like PowerPC is the way they use memory.  RISC provde relatively clumsy memory access, and applications avoid accessing memory.  80x86 access memory in many different ways and applications take advantage of these facilities.  

\subsection{Basic 80x86 Architecture}  

Intel CPU generally classified as a \emph{Von Neumann machine}, containing 3 main building blocks:
\begin{enumerate}
	\item CPU
	\item memory
	\item input/output (I/O) devices
\end{enumerate}
These 3 components are connected together using the \emph{system bus}.  System bus consists of 
\begin{itemize}
	\item address bus
	\item data bus
	\item control bus
\end{itemize}

CPU communicates with memory and I/O devices by placing a numeric value on the address bus to select 1 of the memory locations or I/O device port locations, each of which has a unique binary numeric address.   \\
Then the CPU, I/O, and memory devices pass data among themselves by placing data on the data bus.   \\
Control bus contains signals that determine the direction of data transfer (to or from memory, and to or from an I/O device).  

So 
\[
\begin{gathered}
\begin{gathered}
\textbf{Memory} \\
\text{Obj}{(\textbf{Memory})} = \text{ memory locations } 
\end{gathered} \\ 
\begin{gathered}
\textbf{I/O} \\
\text{Obj}{(\textbf{I/O})} = \text{ I/O device port locations } 
\end{gathered}
\end{gathered}
\]
and 
\[
\begin{gathered}
\textbf{address bus} \\
\text{Obj}{(\textbf{address bus})} = \text{ unique binary numeric addresses } 
\end{gathered}
\]

Likewise, 
\[
\begin{gathered}
\textbf{data bus} \\
\text{Obj}{(\textbf{data bus})} = \text{ data } 
\end{gathered}
\]

And so
\[
\begin{gathered}
\text{CPU} : \textbf{Memory} \times \textbf{address bus} \to \text{Hom}{(\textbf{Memory} , \textbf{address bus}) } \\ 
	\text{CPU}: (\text{memory location} , \text{unique binary numeric address}) \mapsto \&
\end{gathered}
\]
in the language of category theory.  

\subsection{Registers (for 80x86)}  

cf. 3.3 Basic 80x86 Architecture, from pp. 23 of Hyde (2006) \cite{Hyde2006}.  

Example: to add 2 variables, $x,y$ and store $x+y$ into $z$, you must load 1 of the variables into a register, add the 2nd. operand to the register, and then store register's value into destination variable.  "Registers are middlemen in almost every calculation." Hyde (2006) \cite{Hyde2006}.  
    

 There are 4 categories of 80x86 CPU registers: 
 \begin{itemize}
 	\item general-purpose
 	\item special-purpose application-accessible
 	\item segment
 	\item special-purpose kernel-mode
 \end{itemize}
Segment registers not used very much in modern 32-bit operating systems (e.g. Windows, Linux; what about 64-bit?) and special-purpose kernel-mode registers are intended for writing operating systems, debuggers, and other system-level tools.  

\subsubsection{80x86 General-Purpose Registers}  

80x86 CPUs provide several general-purpose registers, including 8 32-bit registers having the following names: (8 bits in 1 byte, 32-bit is 4 bytes)

\[
EAX, EBX, ECX, EDX, ESI, EDI, EBP, ESP
\]
where E is \emph{extended}; 32-bit registers distinguished from 8 16-bit registers with following names 

\[
AX, BX, CX, DX, SI, DI, BP, SP
\]
and finally 

\[
AL, AH, BL, BH, CL, CH, DL, DH
\]
Hyde says that it's important to note about the general-purpose registers is they're not independent.  80x86 doesn't provide 24 separate registers, but overlaps the 32-bit registers with 16-bit registers, and overlaps 16-bit registers with 8-bit registers.  e.g.

\[
\begin{gathered}
\&AL \neq \&AH, \text{ but } \\
\&AL, \&AH \in \&AX \text{and } \&AX \in \&EAX, \text{ so that } \\
\&AL , \&AH, \&AX \in \&EAX
\end{gathered}
\]

\section{Compilers; Compiler Operation and Code Generation}

cf. Ch. 5 Compiler Operation and Code Generation, from pp. 62 and pp. 72- of Hyde (2006) \cite{Hyde2006}


\section{gdp; good debugging processes (in C/C++/CUDA)}

\href{https://youtu.be/heEaKf2b1uA}{Learn C the hard way Lecture 4, Using Debugger (GDB) }


\section{Pointers in C; Pointers in C categorified (interpreted in Category Theory) and its relation to actual, physical, computer memory and (memory) addresses ((address) bus; pointers, structs, arrays in C}

From Shaw (2015) \cite{Shaw2015}, Exercise 15, 

e.g. \verb|ages[i]|, You're indexing into array \verb|ages|, and you're using the number that's held in $i$ to do it:  
\[
\begin{aligned}
& a : \mathbb{Z} \to \text{Type} \in \textbf{Type}  \\
& a: i \mapsto a[i]
\end{aligned} \text{ e.g. } 
\begin{aligned}
& a: \mathbb{Z} \to \mathbb{R} \text{ or } \mathbb{Z}  \\
& a:i \mapsto a[i]
\end{aligned}
\]

Index $i\in \mathbb{Z}$ is a location \emph{inside} \verb|ages| or $a$, which can also be called \emph{address}.  Thus, $a[i]$.  

Indeed, from \href{http://en.cppreference.com/w/cpp/language/operator_member_access}{cppreference for Member access operators}, 

Built-in \emph{subscript} operator provides access to an object pointed-to by pointer or array operand.  And so \verb|E1[E2]| is exactly identical to \verb|*(E1+E2)|.  


To C, e.g. \verb|ages|, or $a$, is a location in computer's memory where, e.g., all these integers (of \verb|ages|) start, i.e. where $a$ starts.  

\[
\begin{aligned}
& \textbf{Memory} , \qquad \, \text{Obj}{(\textbf{Memory})} \ni \text{ memory location.  Also, to specify CPU, } \\ 
& \textbf{Memory}_{CPU} , \qquad \, \text{Obj}{(\textbf{Memory}_{CPU})} \ni \text{ computer memory location }
\end{aligned}
\]

It's \emph{also} an address, and C compiler will replace e.g. \verb|ages| or array $a$, anywhere you type it, with address of very 1st integer (or 1st element) in, e.g. \verb|ages|, or array $a$.  

\[
\begin{gathered}
\begin{aligned}
\textbf{Arrays} & \xleftrightarrow{ \cong } \textbf{address}  \\
\text{Obj}{(\textbf{Arrays} )} & \xleftrightarrow{ \cong } \text{Obj}{(\textbf{address} )}  \\
a & \xleftrightarrow{ \cong } \verb|0x17|  \\
\end{aligned}
\end{gathered}
\]

"But here's the trick": e.g. "\verb|ages| is an address inside the \emph{entire computer}." (Shaw (2015) \cite{Shaw2015}).  

It's not like $i$ that's just an address inside \verb|ages|.  \verb|ages| array name is actually an address in the computer.     

"This leads to a certain realization: C thinks your whole computer is 1 massive array of bytes."  

"What C does is layer on top of this massive array of bytes the concept of types and sizes of those types." (Shaw (2015) \cite{Shaw2015}).  


Let 
\[
\begin{gathered}
\begin{aligned}
& \textbf{Memory}_{CPU} := 1 \text{ massive array of bytes } \\
& \text{Obj}{(\textbf{Memory}_{CPU})} 
\end{aligned}  \qquad \, \qquad \, 
\begin{aligned}
& \textbf{Type} \\
& \text{Obj}{(\textbf{Type})} \ni \verb|int|, \verb|char|, \verb|float| \\
& \text{Obj}{(\textbf{Type})}  \xrightarrow{\texttt{sizeof}} \mathbb{Z}^+  \\
& T \xmapsto{ \texttt{sizeof} } \verb|sizeof|(T) \\
& \verb|float| \xmapsto{ \texttt{sizeof}} \verb|sizeof|(\verb|float|)
\end{aligned}
\end{gathered}
\]


How C is doing the following with your arrays:  

\begin{itemize}
	\item \emph{Create} a block of memory inside your computer:  
	\[
	\text{Obj}{(\textbf{Memory}_{CPU})} \supset \text{ Memory block }  
	\]
	Let $\text{Obj}{(\textbf{Memory}_{CPU})}$ be an ordered set.  Clearly, then memory can be indexed.  Let $b\in \mathbb{Z}^+$ be this index.  Then Memory block(0) $= \text{Obj}{(\textbf{Memory}_{CPU})}(b)$.  		
	\item \emph{Pointing} the name \verb|ages|, or $a$, to beginning of that (memory) block.  
	
	Entertain, possibly, a category of pointers, $\textbf{Pointers} \equiv \textbf{ptrs}$.  
	\[
	\begin{gathered}
	\textbf{ptrs} \\
	\text{Obj}{(\textbf{ptrs})} \ni a, \text{ e.g. } \verb|ages|  
	\end{gathered}
	\]	
	\[
	\begin{gathered}
	a \mapsto \text{Memory block}(0) \\ 
	\text{Obj}{(\textbf{ptrs})} \to \text{Obj}{(\textbf{Memory}_{CPU})}
	\end{gathered}
	\]
	\item \emph{indexing} into the block, by taking the base address of \verb|ages|, or $a$ 
\end{itemize}

\[
\begin{gathered}
\begin{aligned}
&	a \xmapsto{ \cong } \text{ base address $0x17$ } \\
&	\text{Obj}{((T)\textbf{array})} \xrightarrow{ \cong } \text{Obj}{(\textbf{addresses})}  \\	
&	a[i] \equiv a+i \xmapsto{ \cong } \text{ base address + i * } \verb|sizeof|(T)  \xmapsto{ * } a[i] \in T \text{ where $T$, e.g. $T = \mathbb{Z},\mathbb{R}$ } \\ 
& \text{Obj}{((T)\textbf{array}} \xrightarrow{ \cong } \text{Obj}{(\textbf{addresses})} \to T  
\end{aligned}
\end{gathered}
\]

"A pointer is simply an address pointing somewhere inside computer's memory with a type specifier."  Shaw (2015) \cite{Shaw2015}


C knows where pointers are pointing, data type they point at, size of those types, and how to get the data for you.  

\subsubsection{Practical Pointer Usage}
\begin{itemize}
	\item Ask OS for memory block (chunk of memory) and use pointer to work with it.  This includes strings and \verb|structs|.  
	\item Pass by reference - pass large memory blocks (like large structs) to functions with a pointer, so you don't have to pass the entire thing to the function.  
	\item Take the address of a function, for dynamic callback. (function of functions)
\end{itemize}

"You should go with arrays whenever you can, and then only use pointers as performance optimization if you absolutely have to."  Shaw (2015) \cite{Shaw2015}

\subsubsection{Pointers are not Arrays}  

No matter what, pointers and arrays are not the same thing, even though C lets you work with them in many of the same ways.  

From Eli Bendersky's website, \href{https://eli.thegreenplace.net/2009/10/21/are-pointers-and-arrays-equivalent-in-c}{Are pointers and arrays equivalent in C?}

He also emphasizes that 

\subsubsection{Variable names in C are just labels}  

"A variable in C is just a convenient, alphanumeric pseudonym of a memory location." (Bendersky, \cite{Bend}).  What a compiler does is, create label in some memory location, and then access this label instead of always hardcoding the memory value.   

"Well, actually the address is not hard-coded in an absolute way because of loading and relocation issues, but for the sake of this discussion we don't have to get into these details." (Bendersky, \cite{Bend}) (EY : 20171109 so it's on the address bus?)

Compiler assigns label \emph{at compile time}.  Thus, the great difference between arrays and pointers in C.  

\subsubsection{Arrays passed to functions are converted to pointers}  

cf. Bendersky, \cite{Bend}.  

Arrays passed into functions are always converted into pointers.  The argument declaration \verb|char arr_place[]| in  

\begin{lstlisting}
void foo(char arr_arg[], char* ptr_arg)
{
	char a = arr_arg[7];
	char b = ptr_arg[7];
}
\end{lstlisting} 

is just syntactic sugar to stand for \verb|char* arr_place|.  

From Kernighan and Ritchie (1988) \cite{KeRi1988}, pp. 99 of Sec. 5.3, Ch. 5 Pointers and Arrays,  

\begin{quotation}
	When an array name is passed to a function, what is passed is the location of the initial element.  Within the called function, this argument is a local variable, and so an array name parameter is a pointer, that is, a variable containing an address.  
	\end{quotation}

Why?  

The C compiler has no choice here, since, \\
array name is a label the C compiler replaces \emph{at compile time}  with the address it represents (which for arrays is the address of the 1st element of the array).  

But function isn't called at compile time; it's called \emph{at run time}.  \\
At run time, (where) something should be placed on the stack to be considered as an argument.  

Compiler cannot treat array references inside a function as labels and replace them with addresses, because it has no idea what actual array will be passed in at run time.  

EY : 20171109 It can't anticipate the exact arguments that'll it be given \emph{at run-time}; at the very least, my guess is, it's given instructions.  

Bendersky \cite{Bend} concludes by saying the difference between arrays and pointers does affect you.  One way is how arrays can't be manipulated the way pointers can.  Pointer arithmetic isn't allowed for arrays and assignment to an array of a pointer isn't allowed.  cf. van der Linden (1994) \cite{Vand1994}. Ch. 4, 9, 10.  

Bendersky \cite{Bend} has this one difference example, "actually a common C gotcha":  

"Suppose one file contains a global array:"  

\begin{lstlisting}
char my_Arr[256]  
\end{lstlisting}

Programmer wants to use it in another file, \emph{mistakingly} declares as  
\begin{lstlisting}
extern char* my_arr; 
\end{lstlisting}

When he tries to access some element of the array using this pointer, he'll most likely get a segmentation fault or a fatal exception (nomenclature is OS-dependent).  

To understand why, Bendersky \cite{Bend} gave this hint:  look at the assembly listing  

\begin{lstlisting}
    char a = array_place[7];

0041137E  mov  al,byte ptr [_array_place+7 (417007h)]
00411383  mov  byte ptr [a],al

char b = ptr_place[7];

00411386  mov  eax,dword ptr [_ptr_place (417064h)]
0041138B  mov  cl,byte ptr [eax+7]
0041138E  mov  byte ptr [b],cl
\end{lstlisting}

or my own, generated from \verb|gdb| on Fedora 25 Workstation Linux:

\begin{lstlisting}
0x00000000004004b1 <main+11>:	movzbl 0x200b8f(%rip),%eax        # 0x601047 <array_place+7>
0x00000000004004b8 <main+18>:	mov    %al,-0x1(%rbp)

0x00000000004004bb <main+21>:	mov    0x200be6(%rip),%rax        # 0x6010a8 <ptr_place>
0x00000000004004c2 <main+28>:	movzbl 0x7(%rax),%eax
0x00000000004004c6 <main+32>:	mov    %al,-0x2(%rbp)
\end{lstlisting}

"How will the element be accessed via the pointer?  What's going to happen if it's not actually a pointer but an array?"  Bendersky \cite{Bend}   

EY : 20171106.  Instruction-level, the pointer has to     
\begin{itemize}
	\item \verb|mov    0x200be6(%rip),%rax| - 1st., copy value of the pointer (which holds an address), into \verb|%rax| register.  
	\item \verb|movzbl 0x7(%rax),%eax| - off that address in register `%rax`, offset by 7 (or whatever the offset would be) and get value ther with \verb|%eax| register/instruction  
	\item \verb|mov    %al,-0x2(%rbp)| - \verb|mov| contents \verb|-0x2(%rbp)|into register \verb|%al|
\end{itemize}

If it's not actually a pointer, but an array, the value is copied into the \verb|%rax| register is an actual \verb|char| (or \verb|float|, some type).  \emph{Not} an address that the registers may have been expecting!  

\subsection{Structs in C}
From Shaw (2015) \cite{Shaw2015}, Exercise 16, 

\verb|struct| in C is a collection of other data types (variables) that are stored in 1 block of memory.  You can access each variable independently by name.  

\begin{itemize}
	\item The \verb|struct| you make, i.e.g \verb|struct Person| is now a \emph{compound data type}, meaning you can refer to \verb|struct Person| using the same kinds of expressions you would for other (data) types.
	\item This lets you pass the whole \verb|struct| to other functions
	\item You can access individual members of \verb|struct| by their names using \verb|x->y| if dealing with a ptr.  
\end{itemize}  

If you didn't have \verb|struct|, you'd have to figure out the size, packing, and location of memory of the contents.  In C, you'd let it handle the memory structure and structuring of these compound data types, \verb|struct|s.  (Shaw (2015) \cite{Shaw2015})



\part{C, Stack, Heap Memory Management in C}

\section{C, Stack and Heap Memory Management, Heap and Stack Memory Allocation}  

cf. Ex. 17 of Shaw (2015) \cite{Shaw2015}

Consider chunk of RAM called stack, another chunk of RAM called heap.  Difference between heap and stack depends on where you get the storage.  

Heap is all the remaining memory on computer.  Access it with \verb|malloc| to get more.  \\
Each time you call \verb|malloc|, the OS uses internal functions (EY : 20171110 address bus or overall system bus?) to register that piece of memory to you, then returns ptr to it.  

When done, use \verb|free| to return it to OS so OS can use it for other programs.  Failing to do so will cause program to \emph{leak} memory.  (EY: 20171110, meaning this memory is unavailable to the OS?)

Stack, on a special region of memory, stores temporary variables, which each function creates as locals to that function.  How stack works is that each argument to function is \emph{pushed} onto stack and then used inside the function.  Stack is really a stack data structure, LIFO (last in, first out).  This also happens with all local variables in \verb|main|, such as \verb|char action|, \verb|int id|.  The advantage of using stack is when function exits, \emph{C compiler} pops these variables off of stack to clean up.  

Shaw's mantra: If you didn't get it from \verb|malloc|, or a function that got it from \verb|malloc|, then it's on the stack.  

3 primary problems with stacks and heaps: 
\begin{itemize}
	\item If you get a memory block from \verb|malloc|, and have that ptr on the stack, then when function exits, ptr will get popped off and lost.  
	\item If you put too much data on the stack (like large structs and arrays), then you can cause a \emph{stack overflow} and program will abort.  Use the heap with \verb|malloc|.  
	\item If you take a ptr, to something on stack, and then pass or return it from your function, then the function receiving it will \emph{segmentation fault}, because actual data will get popped off and disappear.  You'll be pointing at dead space.  
\end{itemize}
cf. Ex. 17 of Shaw (2015) \cite{Shaw2015}

\section{Data segments; Towards Segmentation Fault}

cf. Ferres (2010) \cite{Ferr2010}

When program is loaded into memory, it's organized into 3 \emph{segments} (3 areas of memory):  
let executable program generated by a compiler (e.g. \verb|gcc|) be organized in memory over a range of addresses (EY : 20171111 assigned to physical RAM memory by address bus?), ordered, from low address to high address.  
\begin{itemize}
	\item \emph{text} segment or code segment - where compiled code of program resides (from lowest address); code segment contains code executable or, i.e. code binary.  

	As a memory region, text segment may be placed below heap, or stack, in order to prevent heaps and stack overflows from overwriting it.  
	
	Usually, text segment is sharable so only a single copy needs to be in memory for frequently executed programs, such as text editors, C compiler, shells, etc.  Also, text segment is often read-only, to prevent program from accidentally modifying its instructions.  	
	cf. \href{http://www.geeksforgeeks.org/memory-layout-of-c-program/}{Memory Layout of C Programs; GeeksforGeeks}  
	\item Data segment:  data segment subdivided into 2 parts:
	\begin{itemize}
		\item initialized data segment - all global, static, constant data stored in data segment. Ferres (2010) \cite{Ferr2010}.  Data segment is a portion of virtual address of a program.  
		
		Note that, data segment not read-only, since values of variables can be altered at run time.  
		
		This segment can also be further classified into initialized read-only area and initialized read-write area.  
		
		e.g. \verb|char s[] = "hello world"| and \verb|int debut = 1| \emph{outside the main (i.e. global)} stored in initialized read-write area.  \\
		\verb|const char *string = "hello world"| in global C statement makes string literal "hello world" stored in initialized read-only area.  Character pointer variable string in initialized read-write area.  
		   cf. \href{http://www.geeksforgeeks.org/memory-layout-of-c-program/}{Memory Layout of C Programs}  
		\item uninitialized data stored in BSS.  Data in this segment is initialized by kernel (OS?) to arithmetic 0 before program starts executing.  
		
		Uninitialized data starts at end of data segment ("largest" address for data segment) and contains all global and static variables initialized to 0 or don't have explicit initialization in source code.  
		
		e.g. \verb|static int i;| in BSS segment.  \\
		e.g. \verb|int j;| global variable in BSS segment. 	
		
		  cf. \href{http://www.geeksforgeeks.org/memory-layout-of-c-program/}{Memory Layout of C Programs}  
			
	\end{itemize}
	\item Heap - "grows upward" (in (larger) address value, begins at end of BSS segment), allocated with \verb|calloc, malloc|, "dynamic memory allocation".    
	
	Heap area shared by all shared libraries and dynamically loaded modules in a process.  
	
	Heap grows when memory allocator invokes \verb|brk()| or \verb|sbrk()| system call, mapping more pages of physical memory into process's virtual address space.  
	
	\item Stack - store local variables, used for passing arguments to functions along with return address of the instruction which is to be executed after function call is over.  
	
	When a new stack frame needs to be added (resulting from a \emph{newly called function}), stack "grows downward."  (Ferres (2010) \cite{Ferr2010})  
	
	Stack grows automatically when accessed, up to size set by kernel (OS?) (which can be adjusted with setrlimit(\verb|RLIMIT_STACK|,$\ldots$ )).  \\
	\end{itemize}

\subsubsection{Mathematical description of Program Memory (data segments), with \textbf{Memory}, \textbf{Addresses}}  

Let $\textbf{Address}$, with $\text{Obj}{(\textbf{Address})} \cong \mathbb{Z}^+$ be an ordered set.  

Memory block $\subset \text{Obj}{(\textbf{Memory})}$, s.t. 
\[
\text{Memory block} \xmapsto{ \cong } \lbrace \text{ low address }, \text{ low address} + \texttt{sizeof}(T) , \dots \text{ high address } \rbrace  \equiv \text{addresses}_{\text{Memory block} } \subset \mathbb{Z}^+
\]
where $T \in \text{Obj}{(\textbf{Types})}$ and $\cong$ assigned by address bus, or the virtual memory table, and $\text{addresses}_{\text{Memory block}} \subset \text{Obj}{(\textbf{Addresses})}$.  

Now, \\
text segment, (initialized) data segment, (uninitialized) data segment, heap, stack, command-line arguments and environmental variables $\subset \text{addresses}_{\text{Memory block}}$, that these so-called data segments are discrete subsets of the set of all addresses assigned for the memory block assigned for the program.  

Now, $\forall \, i \in \text{text segment }, \, \forall \, j \in \text{ (initialized) data segment }$, $i<j$ and $\forall \, j \in \text{ (initialized) data segment }, \, \forall \, k \in \text{ (uninitiaized) data segment }$, $j<k$, and so on.  Let's describe this with the following notation:  

\begin{equation}
\begin{gathered}
\text{text segment} < \text{(initialized) data segment} < \text{(uninitialized) data segment} < \\ 
< \text{heap} < \text{stack} < \text{ command-line arguments and environmental variables}  
\end{gathered}
\end{equation}


Consider stack of variable length $n_{\text{stack}} \in \mathbb{Z}^+$.  Index the stack by $i_{\text{stack}} = 0,1,\dots n_{\text{stack}} - 1$.  "Top of the stack" is towards "decreasing" or "low" (memory) address, so that the relation between "top of stack" to beginning of the stack and high address to low address is \emph{reversed}: 
\[
i_{\text{stack}} \mapsto \text{high address} - i_{\text{stack}}
\]

Call stack is composed of stack frames (i.e. "activation records"), with each stack frame corresponding to a subroutine call that's not yet termined with a routine (at any time).  

The \emph{frame pointer} \verb|FP| points to location where stack pointer was.  

Stack pointer usually is a register that contains the "top of the stack", i.e. stack's "low address" currently, cf. \href{https://www.cs.umd.edu/class/sum2003/cmsc311/Notes/Mips/stack.html}{Understanding the stack}, i.e. 
\begin{equation}
\text{eval}(RSP) = \text{high address} - i_{\text{stack}}
\end{equation}


\subsubsection{Mathematical description of strategy for stack buffer overflow exploitation}

Let $n_{\text{stack}} = n_{\text{stack}}(t)$.  Index the stack with $i_{\text{stack}}$ (from "bottom" of the stack to the "top" of the stack):
\[
0 < i_{\text{stack}} < n_{\text{stack}} - 1
\]
Recall that $i_{\text{stack}} \in \mathbb{Z}^+$ and 
\[
i_{\text{stack}} \mapsto \text{high address } - i_{\text{stack}} \equiv x = x(i_{\text{stack}}) \in \text{Addresses}_{\text{Memory block}} \subset \text{Obj}(\textbf{Address})
\]
Let an array of length $L$ (e.g. \verb|char| array) \verb|buf|, with \verb|\&buf| $=$ \verb|\&buf(0)| $\in  \text{Obj}{(\textbf{Address})}$, be s.t. \verb|&buf| $=x(n_{\text{stack}}-1)$ (starts at "top of the stack and "lowest" address of stack at time $t$, s.t. 
\[
\verb|&buf|(j) = \verb|&buf|(0) + j \verb|sizeof|(T)
\]
with $T\in \textbf{Types}$).  


Suppose return address of a function (such as \verb|main|), $\text{eval}(\verb|RIP|)$ be 
\[
\text{eval}(\verb|RIP|) = \verb|&buf| + L \text{or at least } \text{eval}(\verb|RIP|) \geq \verb|&buf| + L
\]
If we write to \verb|buf| more values than $L$, we can write over \text{eval}(\verb|RIP|), making \text{eval}(\verb|RIP|) a different value than before.  




\subsection{Stack}
cf. Ferres (2010) \cite{Ferr2010}

Stack and functions: When a function executes, it may add some of its state data to top of the stack (EY : 20171111, stack grows downward, so "top" is smallest address?); when function exits, stack is responsible for removing that data from stack.  

In most modern computer systems, each thread has a reserved region of memory, stack.  Thread's stack is used to store location of function calls in order to allow return statements to return to the correct location.  

\begin{itemize}
	\item OS allocates stack for each system-level thread when thread is created.  
	\item Stack is attached to thread, so when thread exits, that stack is reclaimed, vs. heap typically allocated by application at runtime, and is reclaimed when application exits.  
	\item When thread is created, stack size is set.  
	\item Each byte in stack tends to be reused frequently, meaning it tends to be mapped to the processor's cache, making if very fast.  
	\item Stored in computer RAM, \emph{just like} the heap.  
	\item Implemented with an actual stack data structure.  
	\item stores local data, return addresses, used for parameter passing  
	\item Stack overflow, when too much stack is used (mostly from infinite (or too much) recursion, and very large allocation)
	\item Data created on stack can be used without pointers.  
\end{itemize}

Also note, for \textbf{physical location in memory}, because of \href{http://en.wikipedia.org/wiki/Virtual_memory}{Virtual Memory}, makes your program think that you have access to certain addresses where physical data is somewhere else (even on hard disc!).  Addresses you get for stack are in increasing order as your call tree gets deeper.  

\href{http://stackoverflow.com/questions/79923/what-and-where-are-the-stack-and-heap/79988#79988}{memory management - What and where are the stack and heap? Stack Overflow, Tom Leys' answer}

cf. \url{https://stackoverflow.com/questions/79923/what-and-where-are-the-stack-and-heap/80113#80113}  

Stack is memory set aside for a thread of execution.  Each thread gets a stack, while there's typically only 1 heap for the application (although it isn't uncommon to have multiple heaps for different types of allocation).  

"
\begin{quotation}
To what extent are they controlled by the OS or language runtime?
\end{quotation}

The OS allocates the stack for each system-level thread when the thread is created. Typically the OS is called by the language runtime to allocate the heap for the application.

\begin{quotation}
What is their scope?
\end{quotation}

\begin{quotation}
The stack is attached to a thread, so when the thread exits the stack is reclaimed. The heap is typically allocated at application startup by the runtime, and is reclaimed when the application (technically process) exits.
\end{quotation}

\begin{quotation}
What determines the size of each of them?
\end{quotation}

The size of the stack is set when a thread is created. The size of the heap is set on application startup, but can grow as space is needed (the allocator requests more memory from the operating system).




\subsection{Stack overflow}  

If you use heap memory, and you overstep the bounds of your allocated block, you have a decent chance of triggering a segmentation fault (not 100%, since your block may be incidentally contiguous with another that you have previously allocated), says Ferres (2010) \cite{Ferr2010}.  

On stack, since variables created on stack are always contiguous with each other; writing out of bounds can change the value of another variable.  e.g. buffer overflow.  

\subsection{Heap}  

Heap contains a linked list of used and free blocks.  New allocations on the heap (by \verb|new| or \verb|malloc|) are satisfied by creating suitable blocks from free blocks.  

This requires updating list of blocks on the heap.  This meta information about the blocks on the heap is stored \emph{on the heap} often in a small area in front of every block.  

\begin{itemize}
	\item Heap size set on application startup, but can grow as space is needed (allocator requests more memory from OS)
	\item heap, stored in computer RAM, like stack.
\end{itemize}

\subsubsection{Memory leaks}  

Memory leaks occurs when computer program consumes memory, but memory isn't released back to operating system.  

"Typically, a memory leak occurs because dynamically allocated memory becomes unreachable."  (Ferres (2010) \cite{Ferr2010}).  

Programs \verb|./Cmemory/heapstack/Memleak.c| deliberately leaks memory by losing the pointer to allocated memory.  

Note, generally, the OS delays real memory allocation until something is written into it, so program ends when virtual addresses run out of bounds (per process limits).  

\subsection{More Segmentation Faults}  

The operating system (OS) is running the program (its instructions).  Only from the hardware, with \href{https://en.wikipedia.org/wiki/Memory_protection}{memory protection}, with the OS be signaled to a memory access violation, such as writing to read-only memory or writing outside of allotted-to-the-program memory, i.e. data segments.  On \verb|x86_64| computers, this \href{https://en.wikipedia.org/wiki/General_protection_fault}{general protection fault} is initiated by protection mechanisms from the hardware (processor).  From there, OS can signal the fault to the (running) process, and stop it (abnormal termination) and sometimes core dump. 

For \href{https://en.wikipedia.org/wiki/Virtual_memory}{virtual memory}, the memory addresses are mapped by program called \emph{virtual addresses} into \emph{physical addresses} and the OS manages virtual addresses space, hardcare in the CPU called memory management unit (\emph{MMU}) translates virtual addresses to physical addresses, and kernel manages memory hierarchy (eliminating possible overlays).  In this case, it's the \emph{hardware} that detects an attempt to refer to a non-existent segment, or location outside the bounds of a segment, or to refer to location not allowed by permissions for that segment (e.g. write on read-only memory).          

\subsubsection{Dereferencing a ptr to a NULL ptr (in C) at OS, hardware level}

  
The problem, whether it's for dereferencing a pointer that is a null pointer, or uninitialized pointer, appears to (see the \verb|./Cmemory/| subfolder) be at this instruction at the register level:  

\begin{lstlisting}
x000000000040056c <+38>:	movzbl (%rax),%eax

// or 

0x00000000004004be <+24>:	movss  %xmm0,(%rax)  
\end{lstlisting}

involving the register RAX, a temporary register and to return a value, upon assignment.  And in either case, register RAX has trying to access virtual (memory) address \verb|0x0| (to find this out in \verb|gdb|, do \verb|i r| or \verb|info register|).  

Modern OS's run user-level code in a mode, such as \emph{protected mode}, that uses "paging" (using secondary memory source than main memory) to convert virtual addresses into physical addresses.  

For each process (thread?), the OS keeps a \emph{page table} dictating how addresses are mapped.  Page table is stored in memory (and protected, so user-level code can't modify it).  For every memory access, given (memory) address, CPU translates address according to the page table.  

When address translation fails, as in the case that \emph{not all addresses are valid}, and so if a memory access generates an invalid address, the processor (hardware!) raises a \emph{page fault exception}.  "This triggers a transition from \emph{user mode} (aka \emph{current privilege level (CPL) 3} on x86/x86-64) into \emph{kernel mode} (aka CPL 0) to a specific location in the kernel's code, as defined by the \emph{interrupt descriptor table} (IDT)."  cf. [What happens in OS when we dereference a NULL pointer in C?](https://stackoverflow.com/questions/12645647/what-happens-in-os-when-we-dereference-a-null-pointer-in-c)


Kernel regains control and send signal (EY : 20171115 to the OS, I believe).   

In modern OS's, page tables are usually set up to make the address 0 an invalid virtual address.  

cf. [What happens in OS when we dereference a NULL pointer in C?](https://stackoverflow.com/questions/12645647/what-happens-in-os-when-we-dereference-a-null-pointer-in-c)

\part{C++}  

\section{static}



\section{Free Store}  


cf. \href{http://www.gotw.ca/gotw/009.htm}{GotW \#9, Memory Management - Part I}

Free store is 1 of the 2 dynamic memory areas, allocated/freed by new/delete (free store and heap).  Object lifetime can be less than the time the storage is allocated.  

Let $t_{0,\text{obj}}, t_{f,\text{obj}}$, $t_{f,\text{obj}} \geq t_{0,\text{obj}}$, \\
\phantom{Let } $t_{0,\text{alloc}}, t_{f,\text{alloc}}$, $t_{f,\text{alloc}} \geq t_{0,\text{alloc}}$, \\  
with the condition that 
\[
t_{0,\text{alloc}} \leq t_{0,\text{obj}} \leq t_{f,\text{obj}} \leq t_{f,\text{alloc}}
\].  

For $t \in (t_{0,\text{alloc}} , t_{0,\text{obj}} )$, storage may be accessed and manipulated through a \verb|void*| but non of the proto-object's \emph{nonstatic} members or member functions may be accessed, have their addresses taken, or be otherwise manipulated.  




cf. 11.2 Introduction of Ch. 11 Select Operations \cite{Stro2013}.  




\section{Copy vs. Move}  
cf. 17.1 Introduction of Ch. 17 Construction, Cleanup, Copy, and Move of Stroustrup \cite{Stro2013}.  

Difference between \emph{move} and \emph{copy}: after a copy, 2 objects must have same value; whereas after a move, the source of the move isn't required to have its original value.  So moves can be used when source object won't be used again.  

Refs.: Sec. 3.2.1.2, Sec. 5.2, notion of moving a resource, Sec. 13.2-Sec.13.3, object lifetime and errors explored further in Stroustrup \cite{Stro2013}  
  
  
5 situations in which an object is copied or moved:   
\begin{itemize}
	\item as source of an \emph{assignment}
	\item as object initializer 
	\item as function argument
	\item as function return value
	\item as an exception  
\end{itemize}


\subsection{Copy constructor}

cf. \href{http://en.cppreference.com/w/cpp/language/copy_constructor}{Copy constructors, cppreference.com}

Copy constructor of class T is non-template constructor whose 1st parameter is \verb|T&, const T&, volatile T&|, or \verb|const volatile T&|.  

\subsubsection{Syntax}  

\begin{lstlisting}
class_name ( const class_name & )  
class_name ( const class_name & ) = default;  
class_name ( const class_name & ) = delete;
\end{lstlisting}  

\subsubsection{Explanation}  
\begin{enumerate}
\item Typical declaration of a copy constructor.  
\item Forcing copy constructor to be generated by the compiler.  
\item Avoiding implicit generation of copy constructor.  
\end{enumerate}


Copy constructor called whenever an object is \textbf{initialized} (by \textbf{direct-initialization} or \textbf{copy-initialization}) from another object of same type (unless \textbf{overload resolution} selects better match or call is \textbf{elided} (???)), which includes  
\begin{itemize}
\item initialization \verb|T a = b;| or \verb|T a(b);|, where b is of type T;  
\item function argument passing: \verb|f(a);|, where a is of type T and f is \verb|void f(T t)|;  
\item function return: \verb|return a;| inside function such as \verb|T f()|, where a is of type T, which has no \textbf{move constructor}.   
\end{itemize}

\subsubsection{Example}  

\begin{lstlisting}
struct A
{
	int n;
	A(int n = 1) : n(n) { }
	A(const A& a) : n(a.n) { } // user-defined copy ctor
};

struct B : A
{
	// implicit default ctor B::B()
	// implicit copy ctor B::B(const B&)
};

int main()
{
	A a1(7);
	A a2(a1); // calls the copy ctor
	B b;
	B b2 = b;
	A a3 = b; // conversion to A& and copy ctor  
}
\end{lstlisting}  

i.e. cf. \href{http://www.geeksforgeeks.org/copy-constructor-in-cpp/}{Copy Constructor in C++}

\begin{definition}
	\textbf{Copy constructor} is a member function which initializes an object using another object of the same class.  
\end{definition}

\subsubsection{When is copy constructor called? } 
\begin{enumerate}
\item When object of class returned by value 
\item When object of class is passed (to a function) by value as an \textbf{argument}.  
\item When object is constructed based on another object of same class  (or overloaded)  
\item When compiler generates temporary object  
\end{enumerate}


However, it's not guaranteed copy constructor will be called in all cases, because C++ standard allows compiler to optimize the copy away in certain cases.  

\subsubsection{When is used defined copy constructor needed?  shallow copy, deep copy}  

If we don't define our own copy constructor, C++ compiler creates default copy constructor which does member-wise copy between objects.  

We need to define our own copy constructor only if an object has pointers or any run-time allocation of resource like file handle, network connection, etc.  

\subsubsection{Default constructor does only shallow copy.}  

\subsubsection{Deep copy is possible only with user-defined copy constructor.}  

We thus make sure pointers (or references) of copied object point to new memory locations.  

\subsubsection{Copy constructor vs. Assignment Operator}  

\begin{lstlisting}  
MyClass t1, t2; 
MyClass t3 = t1; 	// ----> (1)
t2 = t1; 			// -----> (2)
\end{lstlisting}

Copy constructor called when new object created from an existing object, as copy of existing object, in (1).  
Assignment operator called when already initialized object is assigned a new value from another existing object, as assignment operator is called in (2).  

\subsubsection{Why argument to a copy constructor should be const?  } 

cf. \href{http://www.geeksforgeeks.org/copy-constructor-argument-const/}{Why copy constructor argument should be const in C++?, geeksforgeeks.org}

\begin{enumerate}
\item Use \verb|const| in C++ whenever possible so objects aren't accidentally modified.  
\item e.g.  
\end{enumerate}

\begin{lstlisting}  
#include <iostream>  

class Test
{
	/* Class data members */
	public:
	Test(Test &t) 	{ /* Copy data members from t */ } 
	Test()			{ /* Initialize data members */ }
};

Test fun() 
{
	Test t;
	return t;
};

int main()
{
	Test t1;
	Test t2 = fun();  error: invalid initialization of non-const reference of type Test& from an rvalue of type Test
}
\end{lstlisting}

\verb|fun()| returns by value, so compiler creates temporary object which is copied to t2 using copy constructor (because this temporary object is passed as argument to copy constructor since compiler generates temp. object).  
Compiler error is because \textbf{compiler-created temporary objects cannot be bound to non-const references}. 



\subsection{Move Constructor}  

For a class, to control what happens when we move, or move and assign object of this class type, use special member function \emph{move constructor}, \emph{move-assignment operator}, and define these operations.  Move constructor and move-assignment operator take a (usually nonconst) rvalue reference, to its type.  Typically, move constructor moves data from its parameter into the newly created object.  After move, it must be safe to run the destructor on the given argument.  cf. Ch. 13 of Lippman, Lajole, and Moo (2012) \cite{LLM2012}



\section{vtable; virtual table}  

I was given this answer to a question I posed to a 20 year C++ veteran and it was such an important answer (as I did not know a virtual table existed, at all before), that I will copy this, repeat this and explore this extensively:  

"The keyword you're looking for is virtual table: " \href{https://stackoverflow.com/questions/99297/how-are-virtual-functions-and-vtable-implemented}{How are virtual functions and vtable implemented?, stackoverflow}  

Original question, from \href{https://stackoverflow.com/users/3153/brian-r-bondy}{Brian R. Bondy}:  

\subsection{How are virtual functions and vtable implemented?}

We all know what virtual functions are in C++, but how are they implemented at a deep level?

Can the vtable be modified or even directly accessed at runtime?

Does the vtable exist for all classes, or only those that have at least one virtual function?

Do abstract classes simply have a NULL for the function pointer of at least one entry?

Does having a single virtual function slow down the whole class? Or only the call to the function that is virtual? And does the speed get affected if the virtual function is actually overwritten or not, or does this have no effect so long as it is virtual.

Answer from \emph{community wiki}:  

\subsubsection{How are virtual functions implemented at a deep level?}

From \href{http://wayback.archive.org/web/20100209040010/http://www.codersource.net/published/view/325/virtual_functions_in.aspx}{"Virtual Functions in C++"}

Whenever a program has a virtual function declared, a v - table is constructed for the class. The v-table consists of addresses to the virtual functions for classes that contain one or more virtual functions. The object of the class containing the virtual function contains a virtual pointer that points to the base address of the virtual table in memory.  

Whenever there is a virtual function call, the v-table is used to resolve to the function address.  

An object of the class that contains one or more virtual functions contains a virtual pointer called the vptr at the very beginning of the object in the memory. Hence the size of the object in this case increases by the size of the pointer. This vptr contains the base address of the virtual table in memory.  

Note that virtual tables are class specific, i.e., there is only one virtual table for a class irrespective of the number of virtual functions it contains. This virtual table in turn contains the base addresses of one or more virtual functions of the class. At the time when a virtual function is called on an object, the vptr of that object provides the base address of the virtual table for that class in memory. This table is used to resolve the function call as it contains the addresses of all the virtual functions of that class. This is how dynamic binding is resolved during a virtual function call.

cf. \href{http://wayback.archive.org/web/20100209040010/http://www.codersource.net/published/view/325/virtual_functions_in.aspx}{"Virtual Functions in C++"}

\subsubsection{What is a Virtual Function?}  

A virtual function is a member function of a class, whose functionality can be over-ridden in its derived classes. It is one that is declared as virtual in the base class using the virtual keyword. The virtual nature is inherited in the subsequent derived classes and the virtual keyword need not be re-stated there. The whole function body can be replaced with a new set of implementation in the derived class. 

\subsubsection{What is Binding?}  

Binding is associating an object or a class with its member.   If we call a method \verb|fn()| on an object \verb|o| of a class \verb|c|, we say that object \verb|o| is binded with method \verb|fn()|.  

This happens at \emph{compile time} and is known as \emph{static} - or \emph{compile-time} binding.   

Calls to virtual member functions are resolved during \emph{run-time}.  This mechanisms is known as \emph{dynamic-binding.}   

The most prominent reason why a virtual function will be used is to have a different functionality in the derived class.  The difference between a non-virtual member function and a virtual member function is, the non-virtual member functions are resolved at compile time.  

\subsubsection{How does a Virtual Function work?}  

When a program (code text?) has a virtual function declared, a \textbf{v-table} is \emph{constructed} for the class.  

The v-table consists of addresses to virtual functions for classes that contain 1 or more virtual functions.  \\
The object of the class containing the virtual function \emph{contains a virtual pointer} that points to the base address of the virtual table in memory.  An object of the class that contains 1 or more virtual functions contains a virtual pointer called the \textbf{vptr} at the very beginning of the object in the memory.  (Hence size of the object in this case increases by the size of the pointer; "memory/size overhead.")  \\

This vptr is added as a hidden member of this object.  As such, compiler must generate "hidden" code in the \textbf{constructors} of each class to initialize a new object's vptr to the address of its class's vtable.  

Whenever there's a virtual function call, vtable is used to resolve to the function address.  
This vptr contains base address of the virtual table in memory.  \\

Note that virtual tables are class specific, i.e. there's only 1 virtual table for a class, irrespective of number of virtual functions it contains, i.e.  

vtable is same for all objects belonging to the same class, and typically is shared between them.    
  

This virtual table in turn contains base addresses of 1 or more virtual functions of the class.  

At the time when a virtual function is called on an object, the vptr of that object provides the base address of the virtual table for that class in memory.  This table is used to resolve the function call as it contains the addresses of all the virtual functions of that class.  This is how dynamic binding is resolved during a virtual function call, i.e.  

class (inherited or base/parent) cannot, generally, be determined \emph{statically} (i.e. \textbf{compile-time}), so compiler can't decide which function to call at that (compile) time.  (Virtual function) call must be dispatched to the right function \emph{dynamically} (i.e. \textbf{run-time}).  

  

\subsubsection{Virtual Constructors and Destructors}  

A constructor cannot be virtual because at the time when constructor is invoked, the vtable wouldn't be available in memory.  Hence, we can't have a virtual constructor.  

A virtual destructor is 1 that's declared as virtual in the base class, and is used to ensure that destructors are called in the proper order.  Remember that destructors are called in reverse order of inheritance.  If a base class pointer points to a derived class object, and we some time later use the delete operator to delete the object, then the derived class destructor is not called.  

Finally, the article \href{http://wayback.archive.org/web/20100209040010/http://www.codersource.net/published/view/325/virtual_functions_in.aspx}{"Virtual Functions in C++"} concludes, saying, "Virtual methods should be used judiciously as they are slow due to the overhead involved in searching the virtual table. They also increase the size of an object of a class by the size of a pointer. The size of a pointer depends on the size of an integer."  I will have to check this with other references, because, first of all, how then would class inheritance be otherwise implemented?  

cf. \href{https://stackoverflow.com/questions/99297/how-are-virtual-functions-and-vtable-implemented}{How are virtual functions and vtable implemented?, stackoverflow}  

\subsubsection{Can the vtable be modified or even directly accessed at runtime?  No.}  

"Universally, I believe the answer is "no". You could do some memory mangling to find the vtable but you still wouldn't know what the function signature looks like to call it. Anything that you would want to achieve with this ability (that the language supports) should be possible without access to the vtable directly or modifying it at runtime. Also note, the C++ language spec does not specify that vtables are required - however that is how most compilers implement virtual functions."  

\subsubsection{Do abstract classes simply have a NULL for the function pointer of at least one entry?  Some do place NULL pointer in vtable, some place pointer to dummy method; in general, undefined behavior}

The answer is it is unspecified by the language spec so it depends on the implementation. Calling the pure virtual function results in undefined behavior if it is not defined (which it usually isn't) (ISO/IEC 14882:2003 10.4-2). In practice it does allocate a slot in the vtable for the function but does not assign an address to it. This leaves the vtable incomplete which requires the derived classes to implement the function and complete the vtable. Some implementations do simply place a NULL pointer in the vtable entry; other implementations place a pointer to a dummy method that does something similar to an assertion.

Note that an abstract class can define an implementation for a pure virtual function, but that function can only be called with a qualified-id syntax (ie., fully specifying the class in the method name, similar to calling a base class method from a derived class). This is done to provide an easy to use default implementation, while still requiring that a derived class provide an override.

\subsection{pImpl, shallow copy, deep copy}  

cf. Item 22: "When using the Pimpl Idiom, define special member functions in the implementation file," pp. 147 of Meyers (2014) \cite{ScMe2014}.  

\begin{lstlisting}
class Widget { 			// still in header "widget.h" 
	public:
		Widget();
		~Widget();		// dtor is needed-see below 
		... 
	
	private:
		struct Impl;	// declare implementation struct 
		Impl *pImpl;	// and pointer to it
};
\end{lstlisting}

Because \verb|Widget| no longer mentions types \verb|std::string, std::vector|, and \verb|Gadget, Widget| clients no longer need to \verb|\#include| headers for these types.  That speeds compilation.  

\emph{incomplete type} is a type that has been declared, but not defined, e.g. \verb|Widget::Impl|.  There are very few things you can do with an incomplete type, but declaring a pointer to it is 1 of them.  

\verb|std::unique_ptr|s is advertised as supporting incomplete types.  But, when \verb|Widget w;|, \verb|w|, is destroyed (e.g. goes out of scope), destructor is called and if in class definition using \verb|std::unique_ptr|, we didn't declare destructor, compiler generates destructor, and so compiler inserts code to call destructor for \verb|Widget|'s data member \verb|m_Impl| (or \verb|pImpl|).  

\verb|m_Impl| (or \verb|pImpl|) is a \verb|std::unique_ptr<Widget::Impl>|, i.e., a \verb|std::unique_ptr| using default deleter. The default deleter is a function that uses \verb|delete| on raw pointer inside the \verb|std::unique_ptr|.  Prior to using \verb|delete|, however, implementations typically have default deleter employ C++11's \verb|static_assert| to ensure that raw pointer doesn't point to an incomplete type.  When compiler generates code for the destruction of the \verb|Widget w|, then, it generally encounters a \verb|static_assert| that fails, and that's usually what leads to the error message.  

To fix the problem, you need to make sure that at point where code to destroy \verb|std::unique_ptr<Widget::Impl>| is generated, \verb|Widget::Impl| is a complete type.  The type becomes complete when its definition has been seen, and \verb|Widget::Impl| is defined inside \verb|widget.cpp|.  For successful compilation, have compiler see body of \verb|Widget|'s destructor (i.e. place where compiler will generate code to destroy the \verb|std::unique_ptr| data member) only inside \verb|widget.cpp| after \verb|Widget::Impl| has been defined.  

For compiler-generated move assignment operator, move assignment operator needs to destroy object pointed to by \verb|m_Impl| (or \verb|pImpl|) before reassigning it, but in the \verb|Widget| header file, \verb|m_Impl| (or \verb|pImpl|) points to an incomplete type.  Situation is different for move constructor.  Problem there is that compilers typically generate code to destroy \verb|pImpl| in the event that an exception arises inside the move constructor, and destroying \verb|pImpl| requires \verb|Impl| be complete.  

Because problem is same as before, so is the fix - \emph{move definition of move operations into the implementation file}.  

For copying data members, support copy operations by writing these functions ourselves, because (1) compilers won't generate copy operations for classes with move-only types like \verb|std::unique_ptr| and (2) even if they did, generated functions would copy only the \verb|std::unique_ptr| (i.e. perform a \emph{shallow copy}), and we want to copy what the pointer points to (i.e., perform a \emph{deep copy}).  

If we use \verb|std::shared_ptr|, there'd be no need to declare destructor in \verb|Widget|.  

Difference stems from differing ways smart pointers support custom deleters.  For \verb|std::unique_ptr|, type of deleter is part of type of smart pointer, and this makes it possible for compilers to generate smaller runtime data structures and faster runtime code.  A consequence of this greater efficiency is that pointed-to types must be complete when compiler-generated special functions (e.g. destructors or move operations) are used.  For \verb|std::shared_ptr|, type of deleter is not part of the type of smart pointer.  This necessitates larger runtime data structures and somewhat slower code, but pointed-to types need not be complete when compiler-generated special functions are employed.  

\part{Integers, numeric encodings, number representation, binary representation, hexidecimal representation}

cf. \href{https://www.cs.utexas.edu/users/fussell/courses/cs429h/lectures/Lecture_3-429h.pdf}{Systems 1, Integers lecture at University of Texas, CS429} is a \textbf{very good lecture}; both \emph{mathematically rigorous} and full of useful, \emph{clear} examples.

\section{Unsigned integers}

Consider $\mathbb{Z}^+_{2^w} \to \mathbb{Z}^+$, where $\mathbb{Z}^+_{2^w}$ represent "unsigned" integers. 
\begin{equation}
\begin{gathered}
\mathbb{Z}^+_{2^w} \to \mathbb{Z}^+ \\ 
(x_0, x_1 , \dots , x_{w-1}) \mapsto \sum_{i=0}^{w-1} x_i \cdot 2^i 
\end{gathered}
\end{equation}

For hexidecimal (-based) numbers, observe this relationship: 
\begin{equation}
2^w = 16^v = 2^{4v} \text{ so } w =4v
\end{equation}

So for a hexidecimal number, $v$ hexidecimal numbers represent $w=4v$ bits. 

Observe that 
\begin{equation}
\mathbb{Z}^+_{2^w} = \lbrace 0 , \dots 2^w - 1 \rbrace 
\end{equation}

Also, \textbf{modular addition} $(\mathbb{Z}^+_{2^w}, +)$ forms an \textbf{abelian group}.

\section{Two's complement}

Denote the notation for so-called "two's complement" numbers as $\mathbb{Z}_{2^w}$, which is a representation for integers in such a manner:

\begin{equation}
\begin{gathered}
\mathbb{Z}_{2^w} \to \mathbb{Z} \\
(x_0, x_1, \dots x_{w-1}) \mapsto -x_{w-1} \cdot 2^{w-1} + \sum_{i=0}^{w-2} x_i \cdot 2^i
\end{gathered}
\end{equation}
with
\begin{equation}
\mathbb{Z}_{2^w} = \lbrace - 2^{w-1} \dots 2^{w-1} -1 \rbrace
\end{equation}

Observe that 
\begin{equation}
\begin{gathered}
\max{ \left( \mathbb{Z}^+_{2^w} \right) } = 2 \max{ \mathbb{Z}_{2^w} } + 1 \\
| \min{ \mathbb{Z}_{2^w} } | = \max{ \mathbb{Z}_{2^w} } + 1
\end{gathered}
\end{equation}

\subsection{Negating with complement}

\begin{equation}
\begin{gathered}
\begin{gathered} 
\sim : \mathbb{Z}_{2^w} \to \mathbb{Z} \text{ s.t. } \\
\sim x + 1 = -x \quad \, \forall \, x \in \mathbb{Z}_{2^w} \\
\end{gathered}  \\
\begin{gathered} 
\sim : \mathbb{Z}^+_{2^w} \to \mathbb{Z} \text{ s.t. } \\
\sim x = 2^w- 1 -x 
\end{gathered}
\end{gathered}
\end{equation}

\subsection{Power of 2 Multiply with shift (left shift bitwise operation) }

For both $\mathbb{Z}^+_{2^w}$, $\mathbb{Z}_{2^w}$, 

\begin{equation}
 u << k = u \cdot 2^k \qquad \, \forall \, u \in \mathbb{Z}^+_{2^w} \text{ or } \mathbb{Z}_{2^w} \text{and } k \in \mathbb{Z}
\end{equation}

\subsection{Unsigned power-of-2 Divide, with shift (right shift bitwise operator)}

\begin{equation}
u >> k = \lfloor u / 2^k \rfloor \qquad \, \forall \, u \in \mathbb{Z}^+_{2^w}, \quad \, k \in \mathbb{Z}  
\end{equation}

\section{Endianness} 

Consider \emph{address} $\in \mathbb{Z}^+_{2^3}$.  Consider a \emph{value} $\in \mathbb{Z}^+_{2^3}$ or $\mathbb{Z}_{2^3}$. 

$\forall \, (x_0, x_1, \dots x_{w-1}) \in \mathbb{Z}^+_{2^3} \text{ or } \mathbb{Z}_{2^3}$

Given address $a$, 

\subsection{Big-endian}

\begin{definition}[Big-endian]
	Most-significant byte value is at the lowest address, i.e. 
	\begin{equation}
	\begin{gathered}
	a \mapsto x_{w-1} \\
	a+1 \mapsto x_{w-2} \\ 
	a+2 \mapsto x_{w-3} \\ 
	\vdots \\
	a+w - 1 \mapsto x_0 
	\end{gathered}
	\end{equation}
	\end{definition} 

\subsection{Little-endian}

\begin{definition}[Little-endian]
Least-significant byte value is at the lowest address, i.e.
\begin{equation}
\begin{gathered}
a \mapsto x_0 \\
a + 1 \mapsto x_1 \\
\vdots \\
a+w - 1 \mapsto x_{w-1}
\end{gathered}
\end{equation}

For Little-endian $\to \mathbb{Z}$, \\
$( \text{ addresses } ) \in \mathbb{Z}^+_{2^3} \mapsto \mathbb{Z}$, \\
$a \mapsto (x_0, x_1, \dots x_{w-1})$

For $\mathbb{Z} \to $ Big-endian, \\
$n \mapsto (x_{w-1}, x_{w-2}, \dots x_0)$.

Instead, to tackle the confusing problem of byte ordering, think about the integer as itself first, and then consider \emph{mapping multibyte binary values to memory}.

\begin{equation}
\begin{gathered}
\mathbb{Z} \to \left( \mathbb{Z}_{2^w} \longleftarrow \mathbb{Z}_{(2^3)^w} = \textbf{addresses}  \right) \\
\begin{aligned}
& \mathbb{Z} \ni (x_{w-1}, x_{w-2}, \dots x_1, x_0) \xmapsto{\text{Little-Endian}}  & \left((x_0, x_1 , \dots x_{w-2}, x_{w-1}) \longleftarrow\!\shortmid \,  (a, a+1, \dots a + w-2, a+w-1)\right) \\
& \mathbb{Z} \ni (x_{w-1}, x_{w-2}, \dots x_1, x_0) \xmapsto{\text{Big-Endian}}  & \left((x_{w-1}, x_{w-2} , \dots x_{1}, x_{0}) \longleftarrow\!\shortmid \,  (a, a+1, \dots a + w-2, a+w-1)\right) 
\end{aligned}
\end{gathered}
\end{equation}

 
	
\end{definition}

\part{Linux System Programming}

\section{File Descriptors}

cf. \url{https://medium.com/@copyconstruct/nonblocking-i-o-99948ad7c957}

Let byte $b \in (\mathbb{Z}_2)^8 \equiv B \equiv o$ (when referring to exactly 8 bits).

The fundamental building block of I/O in Unix is a sequence of bytes $(b_n)_{n\in \mathbb{N}}$, $b_n \in B$. 

Most programs work with an even simpler abstraction - a stream of bytes or an I/O stream.

File descriptors $\equiv fd$. $fd$ references I/O streams.  $fd \in \mathbb{N}$ (by definition and the Linux programmer's manual).

\[
\begin{gathered}
\mathbb{N} \to B^{\mathbb{N}} \\
fd \xmapsto{\text{surj}} (b_n)_{n \in \mathbb{N}}
\end{gathered}
\]

e.g. Pipes, files, FIFOs, POSIX IPC's (message queues, semaphores, shared memory), event queues are examples of I/O streams referenced by a fd, i.e. $fd \xmapsto{\text{surj}} (b_n)_{n \in \mathbb{N}}$.

\subsection{Creation and Release of Descriptors}

fd's are either 
\begin{itemize}
	\item explicitly created by system calls like \texttt{open, pipe, socket}, etc. or 
	\item inherited from parent process
\end{itemize}

fd's released when
\begin{itemize}
	\item process exists
	\item calling \texttt{close} system call
	\item implicitly after an \texttt{exec} when fd marked as \textbf{close on exec}; i.e. fd automatically (and atomically) closed when \texttt{exec}-family function succeeds
\end{itemize}

\section{Unix execution model}

cf. \url{https://stackoverflow.com/questions/4204915/please-explain-the-exec-function-and-its-family/37558902}

\subsection{Processes and Programs}

A process is something in which a program executes. $pid \in \mathbb{N} \xmapsto{\text{surj}} \text{ process }$

These 2 operations that you can do is the entire UNIX execution model:

\texttt{fork} - creates new process containing duplicate of current program, including its state. There are differences between the processes so to be able to distinguish which is parent, which is child:

\[
\begin{gathered}
	pid_P \xmapsto{\text{surj}} pid_C > 0 \\
	\text{prog}_i \text{ s.t. } \text{prog}_i \in pid_P \mapsto \text{prog}_i \in pid_C 
\end{gathered}
\]

\texttt{exec} - replaces program in current process with brand new program:
\[
\begin{gathered}
\text{prog}_i \in pid_i \mapsto \text{prog}_{i'} \in pid_i \\
\text{ s.t. } \text{prog}_i = \emptyset, \, \text{prog}_i \neq \text{prog}_{i'}
\end{gathered}
\]

More details:

$\texttt{fork()}$ makes a near duplicate of current process, identical in almost every way; 1 process calls \texttt{fork()} while 2 processes return from it:
\[
pid_P \xmapsto{\texttt{fork()}} pid_P, pid_C >0
\]

Examples:

- When \texttt{fork}, \texttt{exec} used in sequence: 
In shells, \texttt{find}: 
\[
pid_{\text{shell}} \xmapsto{\texttt{fork()}} pid_{\text{shell}}, pid_C \qquad \, \text{prog}_j, pid_C \xmapsto{\texttt{exec()}} \text{prog}_{j'} \in pid_C
\] 

Shell forks, then child loads \texttt{find} program into memory, setting up all command line arguments, standard I/O, etc.

- \texttt{fork()} only.
Daemons simply listening on TCP port and fork copy of themselves to process specific request, while parent goes back to listening. 

Notice that nominally, parent waits for child to exit. 

\textbf{close-on-exec} flag, if set, will have fd automatically be closed during a successful \textbf{execve} (or \texttt{exec}) (cf. \url{http://man7.org/linux/man-pages/man2/fcntl.2.html})

\subsection{File entry}

Every fd points to a data structure called \emph{file entry} in the kernel.

\[
fd \mapsto \text{ file entry }
\]
i.e. $\forall \, fd$, $\exists \,$ \text{ file entry } and pointer to that file entry.


\textbf{open file description} - an entry in system-wide table of open files. \\
open file description records file offset and file status flags.

$n_{\text{offset}} \equiv $ file offset, per fd, from the beginning of file entry.

\texttt{fork()} results in both parent and child processes using same fd and reference to same offset $n_{\text{offset}}$ in file entry.

If process was last to reference file entry, kernel then deallocates that file entry.


Each file entry contains
\begin{itemize}
	\item the type
	\item array of function pointers, that translates generic operations on fds to file-type specific implementations.
\end{itemize}

\texttt{open} call implementation greatly varies for different file types, even if higher level API exposed is the same.

e.g. sockets - Same \texttt{read, write} used for byte-stream type connections, but different system calls handle addressed messages like network datagrams.


For a process pid, consider fd table of fds. $\forall \, $ fd, $fd \mapsto $ open file description $\in$ open file table (system-wide).

$\forall \, $ open file description $\in $ open file table, \\
\qquad \, open file description $= n_{\text{offset}}$, status flags, and \\
\qquad \, open file description $\xmapsto{ \text{ inode ptr } }$ inode table.

A blocking system call is 1 that suspends or puts calling process on wait until an event occurs \footnote{\url{https://stackoverflow.com/questions/19309136/what-is-meant-by-blocking-system-call}, \url{https://www.quora.com/What-is-the-difference-between-a-blocking-system-call-and-an-non-blocking-system-call}}

\subsection{Readiness of fds}

cf. \url{https://medium.com/@copyconstruct/nonblocking-i-o-99948ad7c957}

A fd is considered \emph{ready} if process can perform I/O operation on fd without blocking (i.e. without process having to wait or be suspended). For fd to be considered "ready", it doesn't matter if \textbf{the operation actually transfer any data} - all that matters is that the I/O operation can be performed without blocking (i.e. the calling process won't wait or be suspended).

A fd changes into a \emph{ready} state when an I/O \emph{event} happens, such as the arrival of new input or completion of a socket connection or when space is available on previously full socket send buffer after TCP transmits queued data to the socket peer.

There are 2 ways to find out about the readiness status of a descriptor - edge triggered and level-triggered.

\subsection{Level Triggered}

cf. \url{https://medium.com/@copyconstruct/nonblocking-i-o-99948ad7c957}

To determine if fd is ready, the process tries to perform a non-blocking I/O operation. Also seen as "pull" or "poll" model. (User "pull" on the fd).

At time $t_0$, process pid tries I/O operation on non-blocking fd. If I/O operation blocks, system call returns error \texttt{err}. \\
At time $t_1 > t_0$, pid tries I/O on fd (again). Say call blocks again, and returns \texttt{err}. \\
At time $t_2 > t_1$, pid tries I/O on fd (again). Assume call blocks again, and returns \texttt{err}. \\
At time $t_3 > t_2$, pid polls status of fd, and fd is ready. pid can then choose to actually perform entire I/O operation (e.g. read all data available on socket). \\
At time $t_4 > t_3$, assume pid polls status of fd, and fd isn't ready; call blocks (again), and I/O operation returns \texttt{err}. \\
At time $t_5 > t_4$, pid polls for status of fd, and fd is ready. pid can then choose to only perform a partial I/O operation (e.g., reading only half of all data available) \\
At time $t_6 > t_5$, pid polls status of fd, and fd is ready. This time, pid can choose to perform no I/O.

\subsection{Edge Triggered}

Process $pid$ receives a notification only when fd is "ready" (usually when there's any new activity on fd since it was last monitored). This can be seen as the "push" model, in that a notification is pushed to the process about readiness of a fd.

process pid only notified that fd is ready for I/O, but not provided additional information like for instance how many bytes arrived on socket buffer.

Thus, pid only armed with incomplete data as it tries to perform any subsequent I/O operation 
e.g. 
 at $t_2 > t_0$, pid gets notification about fd being ready.
 
 byte stream $(b_n)_{n\in \mathbb{N}}$ available for I/O stored in buffer.
 
 Assume $N_b = 1024$ bytes available for reading, when pid gets notification at $t= t_2$. 
 \[
 \Longrightarrow (b_n)_{n <N_b} \subset (b_n)_{n\in \mathbb{N}}
 \]
 
Assume pid only reads $N_b' = 500$ bytes $<N_b$

That means for $t=t_3, t_4, \dots t_{i_2}$, there are still $N_b - N_b' = $ 524 bytes available in $(b_n)_{n < N_b}$ that pid can read without blocking.

But since pid can only perform I/O once it gets next notification, these $N_b - N_b'$ bytes remain sitting in buffer for that duration.

Assume pid gets next notification at $t_{i_2} = t_6$, when additional $N_b$ bytes have arrived in buffer.

Then total amount of data available on $(b_n)_{n\in \mathbb{N}} = 2N_b - N_b' = 1548 = 524 + 1024$.

Assume pid reads in $N_b = 1024$ bytes.

This means that at end of 2nd. I/O operation, $N_b - N_b' = 524$ bytes still remain in $(b_n)_{n\in \mathbb{N}}$ that pid won't read before next notification arrives.

While it might be tempting to perform all I/O immediately once notification arrives, doing so has consequences. 
\begin{itemize}
	\item large I/O operation on single fd has potential to starve other fd's 
	\item Furthermore, even with case of level-triggered notifications, an extremely large Write or send call has potential to block.
\end{itemize}

\section{Multiplexing I/O on fds}

In above, we only described how pid handles I/O on a single fd.

Often, pid wants to handle I/O on more than 1 fd.

e.g. program prog needs to log to \texttt{stdout}, \texttt{stderr}, while accept socket connections, and make outgoing RPC connections to other services. 








\end{multicols*}

\begin{thebibliography}{9}
\bibitem{Meye1997}
Bertrand Meyer. \textbf{Object-Oriented Software Construction} (Book/CD-ROM) (2nd Edition) 2nd Edition.   Prentice Hall; 2 edition (April 13, 1997).  ISBN-13: 978-0136291558

\bibitem{Hyde2004}
Randall Hyde.  \textbf{Write Great Code: Volume 1: Understanding the Machine} October 25, 2004.  No Starch Press; 1st edition (October 25, 2004).  ISBN-13: 978-1593270032

\bibitem{Hyde2006}
Randall Hyde.  \textbf{Write Great Code, Volume 2: Thinking Low-Level, Writing High-Level}.   1st Edition.  No Starch Press; 1 edition (March 18, 2006).  ISBN-13: 978-1593270650

\bibitem{Shaw2015}
Zed A. Shaw.  \textbf{Learn C the Hard Way: Practical Exercises on the Computational Subjects You Keep Avoiding (Like C)} (Zed Shaw's Hard Way Series) 1st Edition.  Addison-Wesley Professional; 1 edition (September 14, 2015) ISBN-13: 978-0321884923.  

\bibitem{Bend}
Eli Bendersky.   \href{https://eli.thegreenplace.net/2009/10/21/are-pointers-and-arrays-equivalent-in-c}{Are pointers and arrays equivalent in C?}

\bibitem{KeRi1988}
Brian W. Kernighan, Dennis M. Ritchie.  \textbf{C Programming Language}, 2nd Ed. 1988.   

\bibitem{Vand1994}
Peter van der Linden.  \textbf{Expert C Programming: Deep C Secrets} 1st Edition.  Prentice Hall; 1st edition (June 24, 1994)  ISBN-13: 978-0131774292

\bibitem{Ferr2010}
Leo Ferres.  "Memory management in C: The heap and the stack".  \href{http://www.inf.udec.cl/~leo/teoX.pdf}{Memory management in C: The heap and the stack}


\bibitem{Stro2013}
Bjarne Stroustrup.  \textbf{The C++ Programming Language}, 4th Edition. Addison-Wesley Professional; 4 edition (May 19, 2013).  ISBN-13: 978-0321563842

\bibitem{GhJa1997} 
Carlo Ghezzi, Mehdi Jazayeri.  \textbf{Programming Language Concepts} 3rd Edition.  Wiley; 3 edition (June 23, 1997).  ISBN-13: 978-0471104261  \url{https://vowi.fsinf.at/images/7/72/TU_Wien-Programmiersprachen_VL_(Puntigam)_-_E-Book_SS08.pdf}

\bibitem{LLM2012}
Stanley B. Lippman, Jos\'{e}e Lajoie, Barbara E. Moo. \textbf{C++ Primer} (5th Edition).  Addison-Wesley Professional; 5 edition (August 16, 2012) ISBN-13: 978-0321714114 

\bibitem{ScMe2014}
Scott Meyers.  \textbf{Effective Modern C++: 42 Specific Ways to Improve Your Use of C++11 and C++14}. 1st Edition.  O'Reilly Media; 1 edition (December 5, 2014)  ISBN-13: 978-1491903995


\end{thebibliography}

\end{document}



